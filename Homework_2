val rawblocks = sc.textFile("block_1.csv")
def isHeader(line: String):Boolean={line.contains("id_1")}
val noheader = rawblocks.filter(x => !isHeader(x))
val first = noheader.take(1)
val line = first(0)
val pieces = line.split(',')
val id1 = pieces(0).toInt
val id2 = pieces(1).toInt

def toDouble(s: String) = {
if("?".equals(s)) Double.NaN else s.toDouble
}

def parse(line: String) = {
val pieces = line.split(',')
val id1 = pieces(0).toInt
val id2 = pieces(1).toInt
val middle = pieces.slice(2,11).map(toDouble)
val matched = pieces(11).toBoolean
(id1, id2, middle, matched)
}

val mds = noheader.map(x => parse(x)) //question 4

/*import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}
val vecNoheader = sc.parallelize(Seq(Vectors.dense(mds.map(x))))
val summary: MultivariateStatisticalSummary = Statistics.colStats(vecNoheader)
summary.map(x => println(x))*/




import org.apache.spark.util.StatCounter
class NAStatCounter extends Serializable{
val stats: StatCounter = new StatCounter()
var missing: Long = 0
def add(x: Double): NAStatCounter = {
if(java.lang.Double.isNaN(x)){
missing+=1
} else{
stats.merge(x)
}
this
}
def merge (other: NAStatCounter): NAStatCounter = {
stats.merge(other.stats)
missing += other.missing
this
}
override def toString = {
"stats: " + stats.toString + "NaN: " + missing
}
}

object NAStatCounter extends Serializable{
def apply(x: Double) = new NAStatCounter().add(x)
}


def getStats(line: String) = {
val pieces = line.split(',')
val middle = pieces.slice(2,11).map(toDouble)
val stats = middle.map(d => NAStatCounter(d))
stats
}
val stats = noheader.map(x => getStats(x))
stats.map(x => println(x))
