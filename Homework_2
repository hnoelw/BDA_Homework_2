val rawblocks = sc.textFile("block_1.csv")
def isHeader(line: String):Boolean={line.contains("id_1")}
val noheader = rawblocks.filter(x => !isHeader(x))
val first = noheader.take(1)
val line = first(0)
val pieces = line.split(',')
val id1 = pieces(0).toInt
val id2 = pieces(1).toInt

def toDouble(s: String) = {
if("?".equals(s)) Double.NaN else s.toDouble
}

def parse(line: String) = {
val pieces = line.split(',')
val id1 = pieces(0).toInt
val id2 = pieces(1).toInt
val middle = pieces.slice(2,11).map(toDouble)
val matched = pieces(11).toBoolean
(id1, id2, middle, matched)
}

val mds = noheader.map(x => parse(x)) //question 4

/*import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}
val vecNoheader = sc.parallelize(Seq(Vectors.dense(mds.map(x))))
val summary: MultivariateStatisticalSummary = Statistics.colStats(vecNoheader)
summary.map(x => println(x))*/



import org.apache.spark.util.StatCounter
class NAStatCounter extends Serializable {
val stats: StatCounter = new StatCounter() //immutable
var missing: Long = 0 // mutable
def add(x: Double): NAStatCounter = {
if (java.lang.Double.isNaN(x)) {
missing += 1
} else {
stats.merge(x)
}
this
}
def merge(other: NAStatCounter): NAStatCounter = {
stats.merge(other.stats)
missing += other.missing
this
}
override def toString = {
"stats: " + stats.toString + " NaN: " + missing
}
}
object NAStatCounter extends Serializable {
def apply(x: Double) = new NAStatCounter().add(x)
} // singleton helper function like Java static method


/*def getStats(line: String) = {
val pieces = line.split(',')
val middle = pieces.slice(2,11).map(toDouble)
val stats = middle.map(d => NAStatCounter(d))
}*/

val stats = noheader.map(x => x.map(d => NAStatCounter(d))
